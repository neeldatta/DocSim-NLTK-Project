{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Accuracy of DocSim to TF-IDF (term frequency-inverse document frequency) when Matching Policies to Asset Documentation\n",
    "\n",
    "#### Neel Datta\n",
    "#### July 2021\n",
    "\n",
    "This notebook compares the accuracy of the DocSim nltk tool to the TF-IDF tool to see which returns more accurate results when testing the Policy-Cyber Asset matching tool on one query (in it's entirety and a shortened version of it) with four document links. If the ranking tool works correctly, it should return te cryptography pages with high scores and the other pages with low/near-0% scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/neeldatta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/neeldatta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import docsim\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tfidf import rank_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default GloVe word vector model: glove-wiki-gigaword-50\n",
      "Model loaded\n",
      "Model ready: True\n",
      "CPU times: user 21.6 s, sys: 189 ms, total: 21.8 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docsim_obj = docsim.DocSim(verbose=True)\n",
    "# docsim_obj = docsim.DocSim_threaded(verbose=True)\n",
    "print(f'Model ready: {docsim_obj.model_ready}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in xml file of a list of URLs and converts to string list where each string is a URL\n",
    "def xmlToList(xml): \n",
    "    with open(xml, 'r') as f:\n",
    "        temp = f.read()\n",
    "    temp = re.findall(\"<loc>.*?</loc>\", temp)\n",
    "    strlist = []\n",
    "    for s in temp:\n",
    "        s = s[5:-6]\n",
    "        strlist.append(s)\n",
    "    return strlist\n",
    "    \n",
    "# Function that converts each url into a title + Data node in JSON\n",
    "def htmlToJSON(htmlIn, JSONout):\n",
    "    data = {}\n",
    "    data['data'] = []\n",
    "    for url in htmlIn:\n",
    "        while True:\n",
    "            try:\n",
    "                dpoint = [url]\n",
    "                page = urlopen(url)\n",
    "                html = page.read().decode(\"ISO-8859-1\")\n",
    "                soup = BeautifulSoup(html)\n",
    "                dpoint.append(soup.get_text())\n",
    "                data['data'].append(dpoint)\n",
    "                break\n",
    "            except HTTPError:\n",
    "                print (\"HTTPError at url: \" + url)\n",
    "                break\n",
    "    with open(JSONout, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "def txtToJson(txtIn, JSONout):\n",
    "    data = {}\n",
    "    data['data'] = []\n",
    "    \n",
    "    for url in htmlIn:\n",
    "        while True:\n",
    "            try:\n",
    "                dpoint = [url]\n",
    "                page = urlopen(url)\n",
    "                html = page.read().decode(\"ISO-8859-1\")\n",
    "                soup = BeautifulSoup(html)\n",
    "                dpoint.append(soup.get_text())\n",
    "                data['data'].append(dpoint)\n",
    "                break\n",
    "            except HTTPError:\n",
    "                print (\"HTTPError at url: \" + url)\n",
    "                break\n",
    "    with open(JSONout, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    \n",
    "\n",
    "# Function that converts the list of controls/policies from a JupiterOne PDF into a list of strings\n",
    "def JSONToList(JSONin):\n",
    "    policies = pd.read_json(JSONin)\n",
    "    plist = []\n",
    "    for sec in policies['sections']:\n",
    "        for req in sec['requirements']:\n",
    "            plist.append(sec['title'] + ' ' + req['ref'] + ' : ' + req['title'] + ' : ' + req['summary'])\n",
    "    return plist\n",
    "  \n",
    "    \n",
    "\n",
    "# Function that iterates through n of the controls and compares using docsim with the JSON documents\n",
    "# and outputs file with top 5 matches for each.\n",
    "    # Takes in xml list of all URLs to compare, loads them into input urlJSON files, and outputs a CSV of each control\n",
    "    # and the 5 urls most similar to it\n",
    "    \n",
    "def finalMatches(xmlIn, urlJSON, policyJSON, CSVOut, n):\n",
    "    htmlToJSON(xmlToList(xmlIn), urlJSON)\n",
    "\n",
    "    policies = JSONToList(policyJSON)\n",
    "    \n",
    "#Currently only testing on the first 5 policies for runtime/testing purposes\n",
    "    policies = policies[:n]\n",
    "    \n",
    "# Load test data\n",
    "    with open(urlJSON) as in_file:\n",
    "        urldata = json.load(in_file)\n",
    "    titles = [item[0] for item in urldata['data']]\n",
    "    documents = [item[1] for item in urldata['data']]\n",
    "    print(f'{len(documents)} documents')\n",
    "    \n",
    "# Output findings into CSV file:\n",
    "    with open(CSVOut, mode = 'w') as csvfile:\n",
    "        data = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        data.writerow(['Policy', 'Score', 'URL'])\n",
    "        for p in policies:\n",
    "            query_string = p\n",
    "            similarities = docsim_obj.similarity_query(query_string, documents)\n",
    "            for idx, score in (sorted(enumerate(similarities), reverse=True, key=lambda x: x[1])[:5]):\n",
    "                data.writerow([query_string, str(score), titles[idx]])\n",
    "    return\n",
    "\n",
    "\n",
    "#Function that tests inputted policy string(s) against inputted list of URLs.\n",
    "def testMatches(queries, urls, testJSON):\n",
    "    htmlToJSON(urls, testJSON)\n",
    "\n",
    "    with open(testJSON) as in_file:\n",
    "        urldata = json.load(in_file)\n",
    "    titles = [item[0] for item in urldata['data']]\n",
    "    documents = [item[1] for item in urldata['data']]\n",
    "    print(f'{len(documents)} documents')\n",
    "    \n",
    "    # Test on one string\n",
    "    query_string = queries\n",
    "    similarities = docsim_obj.similarity_query(query_string, documents)\n",
    "\n",
    "    # Output the similarity scores for top 5 documents\n",
    "    for idx, score in (sorted(enumerate(similarities), reverse=True, key=lambda x: x[1])[:5]):\n",
    "        print(f'{idx} \\t {score:0.3f} \\t {titles[idx]}')\n",
    "    return\n",
    "    \n",
    "\n",
    "def testMatchesTFIDF(queries, urls, testJSON):\n",
    "    htmlToJSON(urls, testJSON)\n",
    "\n",
    "    with open(testJSON) as in_file:\n",
    "        urldata = json.load(in_file)\n",
    "    titles = [item[0] for item in urldata['data']]\n",
    "    documents = [item[1] for item in urldata['data']]\n",
    "    print(f'{len(documents)} documents')\n",
    "    \n",
    "    # Test on one string\n",
    "    document_scores = rank_documents(queries, documents)\n",
    "\n",
    "    score_titles = [(score, title) for score, title in zip(document_scores, titles)]\n",
    "\n",
    "    for score, title in (sorted(score_titles, reverse=True, key=lambda x: x[0])[:5]):\n",
    "        print(f'{score:0.3f} \\t {title}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on cryptography procedure. \n",
    "We will now run a search query on key management procedures with the following four web pages:\n",
    "\n",
    "### Should return 0% match:\n",
    "#### https://docs.sumerian.amazonaws.com/tutorials/create/getting-started/light-switch/ \n",
    "#### https://aws.amazon.com/ground-station/\n",
    "\n",
    "### Should return 100% match:\n",
    "#### https://docs.aws.amazon.com/kms/latest/cryptographic-details/intro.html\n",
    "#### https://docs.aws.amazon.com/kms/latest/cryptographic-details/basic-concepts.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents\n",
      "4 documents loaded into corpus\n",
      "1 \t 0.597 \t https://aws.amazon.com/ground-station/\n",
      "2 \t 0.580 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/intro.html\n",
      "3 \t 0.533 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/basic-concepts.html\n",
      "0 \t 0.446 \t https://docs.sumerian.amazonaws.com/tutorials/create/getting-started/light-switch/\n"
     ]
    }
   ],
   "source": [
    "urllist = ['https://docs.sumerian.amazonaws.com/tutorials/create/getting-started/light-switch/', \n",
    "           'https://aws.amazon.com/ground-station/', \n",
    "           'https://docs.aws.amazon.com/kms/latest/cryptographic-details/intro.html',\n",
    "    'https://docs.aws.amazon.com/kms/latest/cryptographic-details/basic-concepts.html'\n",
    "]\n",
    "query = \"The objectives of Key management are: Supporting the users with an existing domain. Generating distribution and installation of keying. Controlling set of Keying material. Storage backup and archival of Keying. The key management techniques are: Symmetric. Key Encryption. Public Key Encryption.\"\n",
    "testMatches(query, urllist, 'test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with smaller query string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents\n",
      "4 documents loaded into corpus\n",
      "3 \t 0.815 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/basic-concepts.html\n",
      "1 \t 0.733 \t https://aws.amazon.com/ground-station/\n",
      "0 \t 0.649 \t https://docs.sumerian.amazonaws.com/tutorials/create/getting-started/light-switch/\n",
      "2 \t 0.637 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/intro.html\n"
     ]
    }
   ],
   "source": [
    "query = \"The key management techniques are: Symmetric. Key Encryption. Public Key Encryption.\"\n",
    "testMatches(query, urllist, 'test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with TF IDF model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents\n",
      "0.561 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/basic-concepts.html\n",
      "0.241 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/intro.html\n",
      "0.003 \t https://aws.amazon.com/ground-station/\n",
      "0.000 \t https://docs.sumerian.amazonaws.com/tutorials/create/getting-started/light-switch/\n"
     ]
    }
   ],
   "source": [
    "testMatchesTFIDF(query, urllist, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents\n",
      "0.368 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/basic-concepts.html\n",
      "0.174 \t https://docs.aws.amazon.com/kms/latest/cryptographic-details/intro.html\n",
      "0.009 \t https://docs.sumerian.amazonaws.com/tutorials/create/getting-started/light-switch/\n",
      "0.004 \t https://aws.amazon.com/ground-station/\n"
     ]
    }
   ],
   "source": [
    "# Testing with longer query again.\n",
    "query = \"The objectives of Key management are: Supporting the users with an existing domain. Generating distribution and installation of keying. Controlling set of Keying material. Storage backup and archival of Keying. The key management techniques are: Symmetric. Key Encryption. Public Key Encryption.\"\n",
    "testMatchesTFIDF(query, urllist, 'test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall, working with TF IDF model on small query strings is the most ideal.\n",
    "As we can see, the TF IDF model correctly assigned zero/near-zero scores to the two random web pages, and significantly higher scores to the two cryptography pages, while the DocSim tool gave them all high scores, and didn't even place the cryptography documentation pages as the two highest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
